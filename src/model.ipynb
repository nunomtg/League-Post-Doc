{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dataset_cleaning\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, CHAMP_TO_IDX = dataset_cleaning.generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def standardize_matchup(blue, red):\n",
    "    blue_str, red_str = '-'.join(map(str, sorted(blue))), '-'.join(map(str, sorted(red)))\n",
    "    \n",
    "    if blue_str < red_str:\n",
    "        return f\"{blue_str}/{red_str}\"\n",
    "    else:\n",
    "        return f\"{red_str}/{blue_str}\"\n",
    "    \n",
    "def standardize_matchup(blue, red):\n",
    "    return '-'.join(map(str, sorted(blue + red)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "   pl.struct([\"Blue_champions\", \"Red_champions\"])\\\n",
    "   .apply(lambda x: standardize_matchup(x['Blue_champions'], x['Red_champions']))\\\n",
    "   .alias('matchup_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34871, 34896)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.n_unique('matchup_id'), df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LeagueOfLegendsDataset(Dataset):\n",
    "    def __init__(self, type=\"train\"):\n",
    "        \n",
    "        assert type in [\"train\", \"test\"]\n",
    "        \n",
    "        self.blue_champions = torch.load(f=f\"dataset\\\\blue_champions_{type}.pt\")\n",
    "        self.red_champions = torch.load(f=f\"dataset\\\\red_champions_{type}.pt\")\n",
    "        self.result = torch.load(f=f\"dataset\\\\result_{type}.pt\")\n",
    "        print(\"Dataset loaded\")\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.blue_champions) == len(self.red_champions) == len(self.result)\n",
    "        return len(self.blue_champions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.blue_champions[idx], self.red_champions[idx], self.result[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n",
      "Dataset loaded\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_train = LeagueOfLegendsDataset(type=\"train\")\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=12, shuffle=True)\n",
    "\n",
    "dataset_test = LeagueOfLegendsDataset(type=\"test\")\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7137, 1784)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamPredictor(nn.Module):\n",
    "    def __init__(self, n_champions, n_dim):\n",
    "        super(TeamPredictor, self).__init__()\n",
    "        \n",
    "        # Define an embedding layer for champion synergies and good-against properties\n",
    "        self.synergy_embeddings = nn.Embedding(n_champions, n_dim)\n",
    "    \n",
    "    def forward(self, blue_team, red_team):\n",
    "        # Embed the blue team champions\n",
    "        blue_team_synergies = self.synergy_embeddings(blue_team)\n",
    "        red_team_synergies = self.synergy_embeddings(red_team)\n",
    "        \n",
    "        # M * M^T, M = (batch, n_champions, dim)\n",
    "        blue_team_synergies = torch.matmul(blue_team_synergies, blue_team_synergies.transpose(1, 2))\n",
    "        red_team_synergies = torch.matmul(red_team_synergies, red_team_synergies.transpose(1, 2))\n",
    "        \n",
    "        # Sum whole everything but the diagonal\n",
    "        final_blue_score = torch.sum(blue_team_synergies, dim=(1, 2)) - torch.sum(torch.diagonal(blue_team_synergies, dim1=1, dim2=2), dim=1)\n",
    "        final_red_score = torch.sum(red_team_synergies, dim=(1, 2)) - torch.sum(torch.diagonal(red_team_synergies, dim1=1, dim2=2), dim=1)\n",
    "        \n",
    "        # Concatenate the blue and red team scores\n",
    "        scores = torch.cat((final_red_score.unsqueeze(-1), final_blue_score.unsqueeze(-1)), dim=1)\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # Import for the scheduler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TeamPredictor(n_champions=len(CHAMP_TO_IDX), n_dim=125).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)  # L2 regularization\n",
    "\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_correct = 0  # Track correct predictions for training\n",
    "    train_samples = 0  # Track total number of training samples\n",
    "    for i, data in enumerate(data_loader_train):\n",
    "        blue_champions, red_champions, labels = data\n",
    "        blue_champions, red_champions, labels = blue_champions.to(device), red_champions.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(blue_champions, red_champions)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_samples += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(data_loader_train)\n",
    "    train_accuracy = 100 * train_correct / train_samples  # Compute training accuracy\n",
    "    print(f\"Epoch: {epoch + 1}\\nTrain Loss: {avg_train_loss:.2f}, Train Accuracy: {train_accuracy:.2f} %\")\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader_test:\n",
    "            blue_champions, red_champions, labels = data\n",
    "            blue_champions, red_champions, labels = blue_champions.to(device), red_champions.to(device), labels.to(device)\n",
    "            outputs = model(blue_champions, red_champions)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            running_test_loss += test_loss.item()\n",
    "\n",
    "    avg_test_loss = running_test_loss / len(data_loader_test)\n",
    "    test_accuracy = 100 * total_correct / total_samples\n",
    "    print(f\"Train Loss: {avg_test_loss:.2f}, Train Accuracy: {test_accuracy:.2f} %\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TeamPredictor_v2(nn.Module):\n",
    "    def __init__(self, n_champions, n_dim, hidden_layers=[256, 128, 64]):\n",
    "        super(TeamPredictor_v2, self).__init__()\n",
    "\n",
    "        # Define an embedding layer for champion synergies and good-against properties\n",
    "        self.embeddings = nn.Embedding(n_champions, n_dim)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "\n",
    "        # A sequence of dense layers\n",
    "        layers = []\n",
    "        input_dim = 10 * n_dim  # 5 champions from each team times the embedding size\n",
    "        \n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            \n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(self.dropout)  # Apply dropout after activation\n",
    "            \n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        # Final layer to get the probability score\n",
    "        layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, blue_team, red_team):\n",
    "        # Embed the blue team and red team champions\n",
    "        blue_team_synergies = self.embeddings(blue_team).view(blue_team.size(0), -1)\n",
    "        red_team_synergies = self.embeddings(red_team).view(red_team.size(0), -1)\n",
    "\n",
    "        # Concatenate the blue and red team champions, so it is (batch_size, n_champions*embedding_dimension*2)\n",
    "        input_tensor = torch.cat((blue_team_synergies, red_team_synergies), dim=1)\n",
    "\n",
    "        scores = self.layers(input_tensor)\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/595 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 237.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.70, Train Accuracy: 51.25 %\n",
      "Test Loss: 0.69, Test Accuracy: 53.03 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 288.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Train Loss: 0.69, Train Accuracy: 52.15 %\n",
      "Test Loss: 0.69, Test Accuracy: 52.97 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 294.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Train Loss: 0.69, Train Accuracy: 53.45 %\n",
      "Test Loss: 0.69, Test Accuracy: 53.53 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 297.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Train Loss: 0.69, Train Accuracy: 52.59 %\n",
      "Test Loss: 0.69, Test Accuracy: 56.39 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 295.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Train Loss: 0.69, Train Accuracy: 53.54 %\n",
      "Test Loss: 0.69, Test Accuracy: 53.48 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 288.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Train Loss: 0.69, Train Accuracy: 54.56 %\n",
      "Test Loss: 0.69, Test Accuracy: 53.70 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 295.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train Loss: 0.69, Train Accuracy: 54.98 %\n",
      "Test Loss: 0.69, Test Accuracy: 55.04 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 286.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train Loss: 0.69, Train Accuracy: 56.07 %\n",
      "Test Loss: 0.69, Test Accuracy: 54.88 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 294.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Train Loss: 0.68, Train Accuracy: 55.61 %\n",
      "Test Loss: 0.69, Test Accuracy: 54.37 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 289.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Train Loss: 0.68, Train Accuracy: 55.77 %\n",
      "Test Loss: 0.69, Test Accuracy: 54.26 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 289.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "Train Loss: 0.68, Train Accuracy: 55.60 %\n",
      "Test Loss: 0.69, Test Accuracy: 54.43 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 292.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "Train Loss: 0.68, Train Accuracy: 56.80 %\n",
      "Test Loss: 0.69, Test Accuracy: 54.93 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 289.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "Train Loss: 0.68, Train Accuracy: 57.17 %\n",
      "Test Loss: 0.69, Test Accuracy: 55.55 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 294.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n",
      "Train Loss: 0.68, Train Accuracy: 58.36 %\n",
      "Test Loss: 0.69, Test Accuracy: 55.94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:02<00:00, 292.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n",
      "Train Loss: 0.67, Train Accuracy: 59.38 %\n",
      "Test Loss: 0.69, Test Accuracy: 54.37 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 382/595 [00:01<00:00, 244.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39msqueeze(), labels\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m     42\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 43\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     44\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     46\u001b[0m \u001b[39m# print(f\"% how_many_time_pred_blue: {how_many_time_pred_blue / len(data_loader_train.dataset)}\")\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m# print(f\"% how_many_time_pred_red: {how_many_time_pred_red / len(data_loader_train.dataset)}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nunom\\anaconda3\\envs\\draft\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nunom\\anaconda3\\envs\\draft\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\nunom\\anaconda3\\envs\\draft\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39meps\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mforeach\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mfused\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgrad_scale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\nunom\\anaconda3\\envs\\draft\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\nunom\\anaconda3\\envs\\draft\\Lib\\site-packages\\torch\\optim\\adam.py:337\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    335\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m--> 337\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(param):\n\u001b[0;32m    338\u001b[0m     grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(grad)\n\u001b[0;32m    339\u001b[0m     exp_avg \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = TeamPredictor_v2(n_champions=len(CHAMP_TO_IDX), n_dim=64).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # L2 regularization\n",
    "\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_correct = 0  # Track correct predictions for training\n",
    "    train_samples = 0  # Track total number of training samples\n",
    "    \n",
    "    how_many_time_pred_blue = 0\n",
    "    how_many_time_pred_red = 0\n",
    "\n",
    "    for i, data in enumerate(tqdm(data_loader_train)):\n",
    "        blue_champions, red_champions, labels = data\n",
    "        blue_champions, red_champions, labels = blue_champions.to(device), red_champions.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(blue_champions, red_champions)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()  # Threshold at 0.5\n",
    "\n",
    "        train_samples += labels.size(0)\n",
    "        train_correct += (predicted.squeeze() == labels.float()).sum().item()\n",
    "            \n",
    "        # how_many_time_pred_blue += (predicted.squeeze() == 1).sum().item()\n",
    "        # how_many_time_pred_red += (predicted.squeeze() == 0).sum().item()\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # print(f\"% how_many_time_pred_blue: {how_many_time_pred_blue / len(data_loader_train.dataset)}\")\n",
    "    # print(f\"% how_many_time_pred_red: {how_many_time_pred_red / len(data_loader_train.dataset)}\")\n",
    "    \n",
    "    avg_train_loss = running_loss / len(data_loader_train)\n",
    "    train_accuracy = 100 * train_correct / train_samples  # Compute training accuracy\n",
    "    print(f\"Epoch: {epoch + 1}\\nTrain Loss: {avg_train_loss:.2f}, Train Accuracy: {train_accuracy:.2f} %\")\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    running_test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader_test:\n",
    "            blue_champions, red_champions, labels = data\n",
    "            blue_champions, red_champions, labels = blue_champions.to(device), red_champions.to(device), labels.to(device)\n",
    "            outputs = model(blue_champions, red_champions)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()  # Threshold at 0.5\n",
    "            \n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted.squeeze() == labels.float()).sum().item()\n",
    "\n",
    "            test_loss = criterion(outputs.squeeze(), labels.float())\n",
    "            running_test_loss += test_loss.item()\n",
    "\n",
    "    avg_test_loss = running_test_loss / len(data_loader_test)\n",
    "    test_accuracy = 100 * total_correct / total_samples\n",
    "    print(f\"Test Loss: {avg_test_loss:.2f}, Test Accuracy: {test_accuracy:.2f} %\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
