{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dataset_cleaning\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, CHAMP_TO_IDX = dataset_cleaning.generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def standardize_matchup(blue, red):\n",
    "    blue_str, red_str = '-'.join(map(str, sorted(blue))), '-'.join(map(str, sorted(red)))\n",
    "    \n",
    "    if blue_str < red_str:\n",
    "        return f\"{blue_str}/{red_str}\"\n",
    "    else:\n",
    "        return f\"{red_str}/{blue_str}\"\n",
    "    \n",
    "def standardize_matchup(blue, red):\n",
    "    return '-'.join(map(str, sorted(blue + red)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "   pl.struct([\"Blue_champions\", \"Red_champions\"])\\\n",
    "   .apply(lambda x: standardize_matchup(x['Blue_champions'], x['Red_champions']))\\\n",
    "   .alias('matchup_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41964, 42140)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.n_unique('matchup_id'), df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LeagueOfLegendsDataset(Dataset):\n",
    "    def __init__(self, type=\"train\"):\n",
    "        \n",
    "        assert type in [\"train\", \"test\"]\n",
    "        \n",
    "        self.blue_champions = torch.load(f=f\"dataset\\\\blue_champions_{type}.pt\")\n",
    "        self.red_champions = torch.load(f=f\"dataset\\\\red_champions_{type}.pt\")\n",
    "        self.result = torch.load(f=f\"dataset\\\\result_{type}.pt\")\n",
    "        print(\"Dataset loaded\")\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.blue_champions) == len(self.red_champions) == len(self.result)\n",
    "        return len(self.blue_champions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.blue_champions[idx], self.red_champions[idx], self.result[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n",
      "Dataset loaded\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_train = LeagueOfLegendsDataset(type=\"train\")\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "dataset_test = LeagueOfLegendsDataset(type=\"test\")\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5987, 1496)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamPredictor(nn.Module):\n",
    "    def __init__(self, n_champions, n_dim):\n",
    "        super(TeamPredictor, self).__init__()\n",
    "        \n",
    "        # Define an embedding layer for champion synergies and good-against properties\n",
    "        self.synergy_embeddings = nn.Embedding(n_champions, n_dim)\n",
    "    \n",
    "    def forward(self, blue_team, red_team):\n",
    "        # Embed the blue team champions\n",
    "        blue_team_synergies = self.synergy_embeddings(blue_team)\n",
    "        red_team_synergies = self.synergy_embeddings(red_team)\n",
    "        \n",
    "        # M * M^T, M = (batch, n_champions, dim)\n",
    "        blue_team_synergies = torch.matmul(blue_team_synergies, blue_team_synergies.transpose(1, 2))\n",
    "        red_team_synergies = torch.matmul(red_team_synergies, red_team_synergies.transpose(1, 2))\n",
    "        \n",
    "        # Sum whole everything but the diagonal\n",
    "        final_blue_score = torch.sum(blue_team_synergies, dim=(1, 2)) - torch.sum(torch.diagonal(blue_team_synergies, dim1=1, dim2=2), dim=1)\n",
    "        final_red_score = torch.sum(red_team_synergies, dim=(1, 2)) - torch.sum(torch.diagonal(red_team_synergies, dim1=1, dim2=2), dim=1)\n",
    "        \n",
    "        # Concatenate the blue and red team scores\n",
    "        scores = torch.cat((final_red_score.unsqueeze(-1), final_blue_score.unsqueeze(-1)), dim=1)\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 29.689509372321925, Train Accuracy: 50.32332700522069 %\n",
      "Test Accuracy: 50.93735168485999 %, Test Loss: 22.02618673178005\n",
      "Epoch: 2, Train Loss: 15.955127936151845, Train Accuracy: 52.51839107736118 %\n",
      "Test Accuracy: 50.735643094447084 %, Test Loss: 12.959794810646173\n",
      "Epoch: 3, Train Loss: 8.374987195878443, Train Accuracy: 54.3812292358804 %\n",
      "Test Accuracy: 50.842429995253916 %, Test Loss: 7.3090223187061145\n",
      "Epoch: 4, Train Loss: 4.172987941292737, Train Accuracy: 55.8999762695776 %\n",
      "Test Accuracy: 50.628856193640246 %, Test Loss: 3.7905131746967107\n",
      "Epoch: 5, Train Loss: 1.9858257189237454, Train Accuracy: 57.24964404366398 %\n",
      "Test Accuracy: 51.649264356905555 %, Test Loss: 1.8197719882855605\n",
      "Epoch: 6, Train Loss: 1.021934645007332, Train Accuracy: 58.13360227812055 %\n",
      "Test Accuracy: 51.74418604651163 %, Test Loss: 0.9902837634425914\n",
      "Epoch: 7, Train Loss: 0.7365514135892548, Train Accuracy: 57.819174181300426 %\n",
      "Test Accuracy: 51.42382534409113 %, Test Loss: 0.7610307197869389\n",
      "Epoch: 8, Train Loss: 0.6911954752602281, Train Accuracy: 56.478405315614616 %\n",
      "Test Accuracy: 53.06122448979592 %, Test Loss: 0.7058565877872808\n",
      "Epoch: 9, Train Loss: 0.6850152935357132, Train Accuracy: 55.33934504034172 %\n",
      "Test Accuracy: 52.266255339345044 %, Test Loss: 0.6925487643853537\n",
      "Epoch: 10, Train Loss: 0.6891779278656517, Train Accuracy: 54.040104413858565 %\n",
      "Test Accuracy: 52.052681537731374 %, Test Loss: 0.6917788439502752\n",
      "Epoch: 11, Train Loss: 0.6918704964054326, Train Accuracy: 52.48576174655909 %\n",
      "Test Accuracy: 52.14760322733745 %, Test Loss: 0.6919082509498669\n",
      "Epoch: 12, Train Loss: 0.6923876896139904, Train Accuracy: 51.595870906502135 %\n",
      "Test Accuracy: 52.883246321784526 %, Test Loss: 0.6922292094076833\n",
      "Epoch: 13, Train Loss: 0.6925972685974584, Train Accuracy: 51.46535358329378 %\n",
      "Test Accuracy: 51.28144280968201 %, Test Loss: 0.6933732344936827\n",
      "Epoch: 14, Train Loss: 0.6927289715523849, Train Accuracy: 51.40009492168961 %\n",
      "Test Accuracy: 50.59326056003797 %, Test Loss: 0.6929697145547089\n",
      "Epoch: 15, Train Loss: 0.6925031765977183, Train Accuracy: 51.33483626008543 %\n",
      "Test Accuracy: 51.388229710488844 %, Test Loss: 0.6925440975791816\n",
      "Epoch: 16, Train Loss: 0.6926678019391455, Train Accuracy: 51.53951115329853 %\n",
      "Test Accuracy: 51.29330802088277 %, Test Loss: 0.692911618009238\n",
      "Epoch: 17, Train Loss: 0.6926414887657311, Train Accuracy: 51.302206929283344 %\n",
      "Test Accuracy: 50.56953013763645 %, Test Loss: 0.6926699666415944\n",
      "Epoch: 18, Train Loss: 0.692680953745034, Train Accuracy: 51.44458946369245 %\n",
      "Test Accuracy: 51.28144280968201 %, Test Loss: 0.6925442070617169\n",
      "Epoch: 19, Train Loss: 0.6926462167690985, Train Accuracy: 51.168723303274795 %\n",
      "Test Accuracy: 51.1390602752729 %, Test Loss: 0.692606379456493\n",
      "Epoch: 20, Train Loss: 0.6924565633892391, Train Accuracy: 51.88063597532036 %\n",
      "Test Accuracy: 50.98481252966303 %, Test Loss: 0.6926257772056609\n",
      "Epoch: 21, Train Loss: 0.6923701422837995, Train Accuracy: 52.02895111532985 %\n",
      "Test Accuracy: 52.218794494542 %, Test Loss: 0.6926449860748349\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 22, Train Loss: 0.69198047130324, Train Accuracy: 53.31929283341243 %\n",
      "Test Accuracy: 52.610346464167066 %, Test Loss: 0.692242599076519\n",
      "Epoch: 23, Train Loss: 0.6918630459379912, Train Accuracy: 53.6811817750356 %\n",
      "Test Accuracy: 52.396772662553396 %, Test Loss: 0.6922300448906263\n",
      "Epoch: 24, Train Loss: 0.6918558696179691, Train Accuracy: 53.69897959183673 %\n",
      "Test Accuracy: 52.36117702895112 %, Test Loss: 0.6921829999737314\n",
      "Epoch: 25, Train Loss: 0.6917242562821764, Train Accuracy: 53.42607973421927 %\n",
      "Test Accuracy: 52.55102040816327 %, Test Loss: 0.6921477743072799\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m train_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted \u001b[39m==\u001b[39m labels)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     36\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 37\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     38\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     40\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\nunom\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\nunom\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # Import for the scheduler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TeamPredictor(n_champions=len(CHAMP_TO_IDX), n_dim=125).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)  # L2 regularization\n",
    "\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_correct = 0  # Track correct predictions for training\n",
    "    train_samples = 0  # Track total number of training samples\n",
    "    for i, data in enumerate(data_loader_train):\n",
    "        blue_champions, red_champions, labels = data\n",
    "        blue_champions, red_champions, labels = blue_champions.to(device), red_champions.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(blue_champions, red_champions)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_samples += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(data_loader_train)\n",
    "    train_accuracy = 100 * train_correct / train_samples  # Compute training accuracy\n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy} %\")\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader_test:\n",
    "            blue_champions, red_champions, labels = data\n",
    "            blue_champions, red_champions, labels = blue_champions.to(device), red_champions.to(device), labels.to(device)\n",
    "            outputs = model(blue_champions, red_champions)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            running_test_loss += test_loss.item()\n",
    "\n",
    "    avg_test_loss = running_test_loss / len(data_loader_test)\n",
    "    print('Test Accuracy: {} %, Test Loss: {}'.format(100 * total_correct / total_samples, avg_test_loss))\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
